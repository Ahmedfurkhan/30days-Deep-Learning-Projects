### Gradient Descent
- **Concept**: An optimization algorithm used to minimize the cost function by iteratively moving towards the minimum.
- **Steps**:
  - Initialize random start point.
  - Update parameters using learning rate and gradient.
  - Repeat until convergence.

### Deep Learning Terminology
- **Learning Rate**: Controls the step size during optimization.
- **Hyperparameters**: Parameters set before training (e.g., learning rate, dropout, batch size).
- **Bias & Variance**: Bias measures how far predictions are from actual values, while variance measures the spread of predictions.

---

## Key Takeaways
- Gradient Descent is a fundamental optimization algorithm in deep learning.
- Learning rate is a critical hyperparameter that affects model convergence.
- Understanding bias and variance helps in diagnosing model performance.

