### Gradient Descent Variations
- **Batch Gradient Descent**: Processes all training examples in each iteration.
- **Stochastic Gradient Descent (SGD)**: Processes one training example per iteration.
- **Mini-Batch Gradient Descent**: Processes small subsets (mini-batches) of the dataset.

### Deep Learning Terminology
- **Overfitting**: Model performs well on training data but poorly on unseen data.
- **Underfitting**: Model performs poorly on both training and unseen data.
- **Epochs**: One full pass through the entire training dataset.
- **Dropout**: A regularization technique to prevent overfitting by randomly dropping neurons during training.

---

## Key Takeaways
- Different gradient descent methods have trade-offs in terms of computational cost and convergence speed.
- Overfitting and underfitting are common challenges in deep learning.
- Dropout is an effective technique to improve model generalization.
