### Activation Functions
- **Linear Activation Function**: Returns the input as output.
- **Sigmoid Activation Function**: Returns values between 0.0 and 1.0, with an S-shaped curve.
- **ReLU Activation Function**: Returns 0 if input is less than 0, otherwise returns the input.
- **Softmax Activation Function**: Converts logits into probabilities, useful for multi-class classification.

### Deep Learning Terminology
- **Batch & Batch Size**: Set of examples used in one iteration of model training.
- **Batch Normalization**: Normalizes input/output of activation functions to stabilize neural networks.
- **Normalization**: Converts data into a standard range (e.g., -1 to +1 or 0 to 1).

---

## Key Takeaways
- Activation functions play a crucial role in determining the output of neural networks.
- Batch normalization helps in stabilizing neural networks and enabling higher learning rates.
- Normalization ensures that data is scaled appropriately for better model performance.

