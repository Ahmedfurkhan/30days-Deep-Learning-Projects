### Optimizers
- **Disadvantages of Gradient Descent**: Slow convergence with small learning rates, instability with large learning rates.
- **Variations of Gradient Descent**:
  - **Momentum**: Accelerates convergence by considering previous gradients.
  - **Nesterov Accelerated Gradient (NAG)**: Improves momentum by looking ahead.
  - **Adagrad**: Adapts learning rates for each parameter.
  - **Adadelta/RMSProp**: Reduces aggressive learning rate decay in Adagrad.
  - **Adam**: Combines the benefits of Adagrad and RMSProp.

### Deep Learning Terminology
- **LSTM (Long Short-Term Memory)**: A type of RNN capable of learning long-term dependencies.
- **Autoencoder**: A neural network that learns efficient data representations by copying input to output.
- **Bagging & Boosting**: Ensemble techniques for improving model performance.

---

## Key Takeaways
- Optimizers like Adam are widely used due to their efficiency and adaptability.
- LSTMs are powerful for sequence-based tasks like time series or NLP.
- Autoencoders are useful for unsupervised learning and data compression.
