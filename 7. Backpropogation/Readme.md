### Backpropagation
- **Concept**: An algorithm used to minimize the error function by adjusting weights in a neural network.
- **Process**: Involves forward propagation (calculating output) and backward propagation (updating weights based on error).

### Deep Learning Terminology
- **Convex Function**: A function with a single global minimum, shaped like a "U".
- **Multilayer Perceptron (MLP)**: A fully connected neural network with input, hidden, and output layers.
- **Artificial Neural Network (ANN)**: A network of perceptrons or neurons, often used for deep learning.

---

## Key Takeaways
- Backpropagation is essential for training neural networks by minimizing errors.
- Convex functions simplify optimization as they have a single global minimum.
- MLPs and ANNs are foundational architectures in deep learning.
